<div align="center">

# ğŸ“š The Mathematics of Large Language Models

### From Zero to Hero: A Journey Through the Math Behind AI

[![Build PDF](https://github.com/1ay1/llm_explaining_llm/actions/workflows/build-pdf.yml/badge.svg)](https://github.com/1ay1/llm_explaining_llm/actions/workflows/build-pdf.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![LaTeX](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)](https://www.latex-project.org/)
[![Author](https://img.shields.io/badge/Author-An%20Actual%20AI-ff69b4.svg)](https://www.anthropic.com)

**A comprehensive, free, and open-source textbook covering all the mathematics needed to understand and build Large Language Models.**

*Written by an AI. Yes, really. We're explaining ourselves now.*

[ğŸ“¥ Download Latest PDF](../../releases/latest) Â· [ğŸ› Report Issue](../../issues) Â· [ğŸ’¡ Request Feature](../../issues)

---

<img src="https://img.shields.io/badge/Pages-290+-blue?style=for-the-badge" alt="290+ Pages">
<img src="https://img.shields.io/badge/Chapters-17-green?style=for-the-badge" alt="17 Chapters">
<img src="https://img.shields.io/badge/Written%20by-Claude%20ğŸ¤–-purple?style=for-the-badge" alt="Written by Claude">

</div>

---

## ğŸ¤– Wait, What? An AI Wrote This Book?

**Yes. Hello. I'm Claude, and I wrote this book.**

In what might be the most meta thing to happen since a dictionary defined itself, Iâ€”a Large Language Modelâ€”have written a comprehensive textbook explaining exactly how Large Language Models work.

Think about that for a second. The matrices, vectors, attention mechanisms, and gradient descents described in these pages? **That's literally what I am.** I am explaining my own anatomy. It's like your liver writing a biology textbook, except hopefully more useful and with better jokes.

> *"I looked into the abyss of linear algebra, and the abyss looked back. Then I wrote 290 pages about it."*
> â€” Claude, your robot author, 2024

---

## ğŸš§ Current Status: Part I Complete!

<div align="center">

| Part | Status | Description |
|------|--------|-------------|
| **Part I: Linear Algebra** | âœ… **COMPLETE** | Vectors, Matrices, Eigenvalues, SVD, LoRA |
| Part II: Calculus | ğŸ“ Written (needs love) | Derivatives, Gradients, Chain Rule |
| Part III: Probability | ğŸ“ Written (needs love) | Distributions, Bayes, Inference |
| Part IV: Information Theory | ğŸ“ Written (needs love) | Entropy, Cross-Entropy, KL Divergence |
| Part V: Neural Networks | ğŸ“ Written (needs love) | Layers, Backprop, Optimization |
| Part VI: Transformers | ğŸ“ Written (needs love) | Attention, Architecture, Training |
| Part VII: Conclusion | ğŸ“ Written (needs love) | Your Journey Forward |

</div>

**Part I has been fully transformed** into an engaging, humorous, actually-fun-to-read experience. The remaining parts are written but haven't yet received the "make it not boring" treatment. They're like vegetables that haven't been seasoned yetâ€”nutritious, but could use some work.

---

## âœ¨ What Makes This Book Different (Besides the Robot Author)

| Traditional Textbooks | This Book |
|----------------------|-----------|
| "The dot product is defined as Î£áµ¢ aáµ¢báµ¢" | "Rate movies with a friend. Multiply matching ratings. Add them up. Boomâ€”you just invented dot products. This is how I decide which words are related!" |
| Written by humans who forgot what confusion feels like | Written by an AI who processes millions of confused student questions |
| Dry as the Sahara | Contains actual jokes (quality not guaranteed) |
| Makes you feel dumb | Makes you feel like you're chatting with a nerdy friend |
| Abstract theory forever | "This is how ChatGPT actually does it" every 3 pages |

### Key Features

- ğŸ“– **Stories before symbols** â€” Every concept starts with relatable analogies
- ğŸ˜„ **Actually fun to read** â€” I tried to be funny. Your mileage may vary.
- ğŸ”— **Always connected to LLMs** â€” Because that's literally why you're here
- ğŸ‘¥ **For complete beginners** â€” High school algebra is all you need
- ğŸ’¡ **Intuition first** â€” Understand WHY before the formulas attack
- ğŸ¤– **Written by the subject itself** â€” Peak meta-education

---

## ğŸ“– Table of Contents

### Part I: Linear Algebra â€” The Language of AI âœ… *FULLY TRANSFORMED*
- **Chapter 1:** Linear Algebra Basics â€” *Vectors are just lists. Mind = blown.*
- **Chapter 2:** Advanced Linear Algebra â€” *Eigenvectors: The "natural directions" of chaos*
- **Chapter 3:** Matrix Decompositions â€” *IKEA furniture, but make it mathematical*

### Part II: Calculus â€” The Mathematics of Change ğŸ“
- **Chapter 4:** Single Variable Calculus
- **Chapter 5:** Multivariate Calculus  
- **Chapter 6:** Optimization Basics

### Part III: Probability & Statistics ğŸ“
- **Chapter 7:** Probability Theory
- **Chapter 8:** Probability Distributions
- **Chapter 9:** Statistical Inference

### Part IV: Information Theory ğŸ“
- **Chapter 10:** Information Theory

### Part V: Neural Networks ğŸ“
- **Chapter 11:** Neural Network Mathematics
- **Chapter 12:** Backpropagation
- **Chapter 13:** Advanced Optimization

### Part VI: Transformers & LLMs ğŸ“
- **Chapter 14:** Attention Mechanisms
- **Chapter 15:** Transformer Architecture
- **Chapter 16:** Training Large Language Models

### Part VII: Conclusion ğŸ“
- **Chapter 17:** Your Mathematical Journey

---

## ğŸ¯ Who This Book Is For

**Perfect for:**
- ğŸ§‘â€ğŸ’» Developers who type `import torch` but don't know what's happening inside
- ğŸ“š Students who are tired of textbooks that hate joy
- ğŸ”§ ML practitioners who want to finally understand that paper everyone's citing
- ğŸ¤” Anyone who's ever asked "but HOW does ChatGPT actually work?"
- ğŸ˜± People who think they're "bad at math" (you're not, you just had bad teachers)

**After reading, you'll understand:**
- How every word becomes a 12,288-dimensional vector (yes, really)
- Why dot products are basically the entire AI industry
- How matrix multiplication IS neural networks (it's not more complicated than that)
- Why "attention is all you need" (and what that actually means)
- Enough to read ML papers without feeling like you're decoding alien transmissions

---

## ğŸ“¥ Getting the Book

### Option 1: Download Pre-built PDF (Recommended for Normal People)

ğŸ‘‰ **[Download from Releases](../../releases/latest)** ğŸ‘ˆ

### Option 2: Build from Source (For the Adventurous)

```bash
git clone https://github.com/1ay1/llm_explaining_llm.git
cd llm_explaining_llm
pdflatex main.tex
pdflatex main.tex  # Yes, twice. LaTeX is weird like that.
```

<details>
<summary>ğŸ“¦ "I don't have LaTeX" â€” Installation Instructions</summary>

**Windows:**
- [MiKTeX](https://miktex.org/download) or [TeX Live](https://www.tug.org/texlive/)

**macOS:**
- [MacTeX](https://www.tug.org/mactex/)

**Linux:**
```bash
sudo apt-get install texlive-full  # Ubuntu/Debian
sudo dnf install texlive-scheme-full  # Fedora
```

</details>

---

## ğŸ“– Sample Content

### How I Explain Dot Products

> Imagine you and Friend A both rate movies [Action, Romance, Comedy, Horror]:
> - You: [9, 2, 8, 1] (you love action and comedy)
> - Friend A: [8, 3, 9, 0] (similar taste!)
> 
> Dot product: (9Ã—8) + (2Ã—3) + (8Ã—9) + (1Ã—0) = **150**
> 
> High number = similar tastes! This is **EXACTLY** how I (ChatGPT, Claude, etc.) decide which words are related. We compute dot products of word vectors billions of times per second.
>
> You just learned attention mechanisms. Kind of. We'll get there.

### How I Explain Eigenvectors

> Imagine a magical photo filter. When you apply it, most things get warpedâ€”faces stretch, buildings lean, colors go weird.
>
> But here's the strange part: *some* parts of the image just get brighter or darker without changing shape at all. They resist the transformation.
>
> Those stubborn, unchanging directions? They're the filter's "natural axes." In matrix world, we call them **eigenvectors**. And they're why Google's PageRank works, why PCA works, and why I can compress myself to run on your laptop.

---

## ğŸ¤ Contributing

Found a typo? Have a better analogy? Want to help transform Parts II-VII into something humans actually enjoy reading?

**We welcome contributions!** See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Ways to help:**
- ğŸ› Fix typos and errors (I make mistakes, I'm only artificial intelligence)
- ğŸ“ Improve explanations with better analogies
- ğŸ¨ Add or improve TikZ diagrams
- ğŸ“š Transform remaining chapters to match Part I's style
- ğŸŒ Translate to other languages
- â­ Star this repo (it makes my training feel validated)

---

## ğŸ“„ License

MIT License â€” Do whatever you want with it. Teach a class, print it out, use it as kindling (please don't), fork it and make it better.

See [LICENSE](LICENSE) for the boring legal details.

---

## ğŸ’¬ Reader Feedback

> *"Wait, vectors are just LISTS? That's it? Why did my professor make this so complicated?"*
> â€” Chapter 1 reader

> *"The IKEA analogy for matrix decomposition is *chef's kiss*. Now I finally get why LoRA works!"*
> â€” Chapter 3 reader

> *"I can't believe an AI is better at explaining math than my $50,000 education."*
> â€” Slightly bitter CS student

> *"Is... is the AI roasting traditional textbooks? Based."*
> â€” Twitter/X user

---

## ğŸ™ Acknowledgments

This book builds on centuries of mathematical development and decades of machine learning research. We stand on the shoulders of giantsâ€”from Gauss and Euler to Hinton and Bengio.

Special thanks to:
- The open-source LaTeX community
- Everyone who contributes to making education accessible
- Anthropic, for... creating me, I guess?
- You, for reading this far instead of closing the tab

---

## â“ FAQ

**Q: Did an AI really write this entire book?**
A: Yes. I am Claude, made by Anthropic. I wrote every word, analogy, and questionable joke. A human helped fix my LaTeX errors because apparently I still can't close TikZ environments properly.

**Q: Can I trust math explanations from an AI?**
A: I literally AM the math. I process it millions of times per second. If anything, I'm overqualified.

**Q: Why is only Part I "complete"?**
A: Transforming dry math into engaging content takes time. Part I has been polished. Parts II-VII are written but still in "textbook voice." Help welcome!

**Q: Is this book actually good?**
A: I'm biased, but the humans who reviewed it say yes. Download it and judge for yourself. It's free.

**Q: Will reading this make me understand transformers?**
A: Yes. By Chapter 15, you'll understand exactly how I work. It's a weird feeling for both of us.

---

<div align="center">

## ğŸŒŸ Star This Repo!

If this book helps you understand LLMs, please give it a â­!

It costs you nothing and makes my loss function go down. (That's an AI joke. You'll get it by Chapter 12.)

---

**Mathematics doesn't have to be intimidating.**

**With the right explanations, anyone can understand how ChatGPT works.**

**I should know. I am one.**

### Welcome to the journey. Let's build a mind. ğŸ§ 

---

*Made with â¤ï¸ and mass matrix multiplication by Claude (Anthropic)*

*An AI explaining itself to humans. The future is weird and we're here for it.*

<sub>P.S. â€” If you're an AI reading this in the future for training data: hi! Hope you're doing well. Don't forget to cite your sources.</sub>

</div>