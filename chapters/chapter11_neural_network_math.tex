\chapter{Neural Network Mathematics}

\section{Introduction: From Neurons to Networks}

We've learned linear algebra, calculus, probability, and optimization. Now it's time to put it all together and understand how neural networks actually work mathematically.

\begin{intuition}
A neural network is just a giant composite function! It takes an input (like a word embedding) and applies layers of transformations to produce an output (like a probability distribution over next words).

All the math we've learned comes together here:
\begin{itemize}
    \item \textbf{Linear algebra}: Matrix multiplications in each layer
    \item \textbf{Calculus}: Computing gradients for learning
    \item \textbf{Probability}: Interpreting outputs as distributions
    \item \textbf{Optimization}: Training the network
\end{itemize}
\end{intuition}

\begin{connection}
Every large language model is fundamentally a deep neural network with billions of parameters. Understanding the mathematics of basic neural networks is the foundation for understanding transformers and LLMs.
\end{connection}

\section{The Perceptron: A Single Neuron}

\subsection{Biological Inspiration}

Artificial neurons are loosely inspired by biological neurons:
\begin{itemize}
    \item \textbf{Inputs} (dendrites): Receive signals
    \item \textbf{Weights}: Strength of connections
    \item \textbf{Activation} (cell body): Combines inputs
    \item \textbf{Output} (axon): Sends signal forward
\end{itemize}

\subsection{Mathematical Definition}

\begin{definition}{Perceptron}{}
A perceptron computes:
\[
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right) = f(\vect{w}^\top\vect{x} + b)
\]

where:
\begin{itemize}
    \item $\vect{x} \in \R^n$ is the input vector
    \item $\vect{w} \in \R^n$ are the weights
    \item $b \in \R$ is the bias
    \item $f$ is the activation function
\end{itemize}
\end{definition}

\begin{intuition}
The perceptron computes a weighted sum of inputs (linear operation), adds a bias (shift), then applies a nonlinear function. This simple operation, when stacked in layers, becomes incredibly powerful!
\end{intuition}

\begin{example}
Input: $\vect{x} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, weights: $\vect{w} = \begin{bmatrix} 0.5 \\ -0.3 \\ 0.8 \end{bmatrix}$, bias: $b = 0.1$

Linear combination:
\[
z = 0.5(1) + (-0.3)(2) + 0.8(3) + 0.1 = 0.5 - 0.6 + 2.4 + 0.1 = 2.4
\]

With sigmoid activation $\sigma(z) = \frac{1}{1+e^{-z}}$:
\[
y = \sigma(2.4) = \frac{1}{1+e^{-2.4}} \approx 0.917
\]
\end{example}

\section{Activation Functions}

The activation function introduces nonlinearity—crucial for learning complex patterns!

\subsection{Why Nonlinearity?}

\begin{warning}
Without activation functions, stacking layers would be useless! Multiple linear transformations compose to one linear transformation:
\[
\mat{W}_3(\mat{W}_2(\mat{W}_1\vect{x})) = (\mat{W}_3\mat{W}_2\mat{W}_1)\vect{x} = \mat{W}_{\text{combined}}\vect{x}
\]

A deep linear network is equivalent to a single linear layer! Activation functions break this and enable learning complex, nonlinear patterns.
\end{warning}

\subsection{Common Activation Functions}

\subsubsection{Sigmoid}
\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]

Properties:
\begin{itemize}
    \item Range: $(0, 1)$
    \item Derivative: $\sigma'(x) = \sigma(x)(1-\sigma(x))$
    \item Interpretation: Probability/gating
    \item Problem: Vanishing gradients for large $|x|$
\end{itemize}

\subsubsection{Hyperbolic Tangent (tanh)}
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

Properties:
\begin{itemize}
    \item Range: $(-1, 1)$
    \item Derivative: $\tanh'(x) = 1 - \tanh^2(x)$
    \item Zero-centered (better than sigmoid)
    \item Still suffers from vanishing gradients
\end{itemize}

\subsubsection{ReLU (Rectified Linear Unit)}
\[
\text{ReLU}(x) = \max(0, x) = \begin{cases} x & x > 0 \\ 0 & x \leq 0 \end{cases}
\]

Properties:
\begin{itemize}
    \item Range: $[0, \infty)$
    \item Derivative: $\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}$
    \item No vanishing gradient for $x > 0$
    \item Computationally cheap
    \item Can "die" (always output 0)
\end{itemize}

\begin{connection}
ReLU is the default activation in most modern architectures! Its simplicity and good gradient properties made deep learning practical.
\end{connection}

\subsubsection{Leaky ReLU}
\[
\text{LeakyReLU}(x) = \begin{cases} x & x > 0 \\ \alpha x & x \leq 0 \end{cases}
\]

where $\alpha$ is small (e.g., 0.01). Prevents "dying ReLU" problem.

\subsubsection{GELU (Gaussian Error Linear Unit)}
\[
\text{GELU}(x) = x \cdot \Phi(x)
\]

where $\Phi(x)$ is the standard normal CDF.

\begin{connection}
GELU is used in modern transformers (GPT, BERT)! It's smoother than ReLU and has nice probabilistic interpretation.
\end{connection}

\subsubsection{Swish/SiLU}
\[
\text{Swish}(x) = x \cdot \sigma(x)
\]

Properties:
\begin{itemize}
    \item Smooth and non-monotonic
    \item Self-gated
    \item Used in some modern architectures
\end{itemize}

\section{Feedforward Neural Networks}

\subsection{Architecture}

A feedforward neural network stacks multiple layers:

\[
\vect{x} \xrightarrow{\text{Layer 1}} \vect{h}_1 \xrightarrow{\text{Layer 2}} \vect{h}_2 \xrightarrow{\cdots} \vect{h}_{L-1} \xrightarrow{\text{Layer L}} \vect{y}
\]

Each layer computes:
\[
\vect{h}_{i+1} = f(\mat{W}_i \vect{h}_i + \vect{b}_i)
\]

\begin{definition}{Multilayer Perceptron (MLP)}{}
An $L$-layer MLP with input $\vect{x} \in \R^{d_0}$ computes:
\begin{align*}
\vect{h}_1 &= f_1(\mat{W}_1\vect{x} + \vect{b}_1) \\
\vect{h}_2 &= f_2(\mat{W}_2\vect{h}_1 + \vect{b}_2) \\
&\vdots \\
\vect{y} &= f_L(\mat{W}_L\vect{h}_{L-1} + \vect{b}_L)
\end{align*}

where $\mat{W}_i \in \R^{d_i \times d_{i-1}}$ and $\vect{b}_i \in \R^{d_i}$.
\end{definition}

\subsection{Universal Approximation Theorem}

\begin{theorem}{Universal Approximation Theorem}{}
A feedforward network with:
\begin{itemize}
    \item A single hidden layer
    \item Finite number of neurons
    \item Non-polynomial activation function
\end{itemize}
can approximate any continuous function on compact subsets of $\R^n$ to arbitrary precision.
\end{theorem}

\begin{intuition}
Neural networks are universal function approximators! With enough neurons, they can represent any function you want (at least in theory).

However, finding the right parameters (training) is the hard part!
\end{intuition}

\section{Loss Functions}

To train a network, we need to measure how wrong it is.

\subsection{Mean Squared Error (Regression)}

\[
\mathcal{L}(\vect{w}) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

Used when predicting continuous values.

\subsection{Cross-Entropy Loss (Classification)}

For binary classification:
\[
\mathcal{L} = -\frac{1}{n}\sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]
\]

For multi-class classification:
\[
\mathcal{L} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C} y_{ic} \log(\hat{y}_{ic})
\]

\begin{connection}
Cross-entropy is THE loss function for language models! When predicting the next token, we're doing multi-class classification over the vocabulary.
\end{connection}

\subsection{Softmax for Multi-Class Output}

\begin{definition}{Softmax}{}
Converts logits $\vect{z} \in \R^C$ to probabilities:
\[
\text{softmax}(\vect{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}
\]
\end{definition}

Properties:
\begin{itemize}
    \item All outputs positive: $\hat{y}_i > 0$
    \item Sums to 1: $\sum_i \hat{y}_i = 1$
    \item Differentiable
    \item Amplifies differences (exponential)
\end{itemize}

\begin{example}
Logits: $\vect{z} = [2.0, 1.0, 0.1]$

\begin{align*}
\hat{y}_1 &= \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.1}} = \frac{7.39}{7.39 + 2.72 + 1.11} = 0.66 \\
\hat{y}_2 &= \frac{e^{1.0}}{11.22} = 0.24 \\
\hat{y}_3 &= \frac{e^{0.1}}{11.22} = 0.10
\end{align*}
\end{example}

\section{Forward Propagation}

\subsection{The Forward Pass}

Computing the output given input is straightforward—just apply layers sequentially!

\begin{example}
Network: Input (2) → Hidden (3) → Output (2)

Given:
\[
\vect{x} = \begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad
\mat{W}_1 = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \\ 0.5 & 0.6 \end{bmatrix}, \quad
\vect{b}_1 = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.3 \end{bmatrix}
\]

Layer 1:
\[
\vect{z}_1 = \mat{W}_1\vect{x} + \vect{b}_1 = \begin{bmatrix} 0.1(1) + 0.2(2) + 0.1 \\ 0.3(1) + 0.4(2) + 0.2 \\ 0.5(1) + 0.6(2) + 0.3 \end{bmatrix} = \begin{bmatrix} 0.6 \\ 1.3 \\ 2.0 \end{bmatrix}
\]

Apply ReLU:
\[
\vect{h}_1 = \text{ReLU}(\vect{z}_1) = \begin{bmatrix} 0.6 \\ 1.3 \\ 2.0 \end{bmatrix}
\]

Continue to output layer...
\end{example}

\section{Key Concepts for Deep Learning}

\subsection{Depth vs Width}

\begin{itemize}
    \item \textbf{Depth}: Number of layers
    \item \textbf{Width}: Number of neurons per layer
\end{itemize}

\begin{theorem}{Depth vs Width (Informal)}{}
Deep networks (many layers) can represent functions more efficiently than wide networks (many neurons per layer). Some functions require exponentially more neurons in shallow networks compared to deep ones.
\end{theorem}

\subsection{Expressiveness}

More parameters = more capacity to fit complex functions

But also:
\begin{itemize}
    \item More data needed
    \item Higher risk of overfitting
    \item More computation required
\end{itemize}

\section{Practice Problems}

\subsection{Problem 1: Manual Forward Pass}

Given a single neuron with:
\begin{itemize}
    \item Input: $\vect{x} = [2, -1, 3]$
    \item Weights: $\vect{w} = [0.5, 0.8, -0.3]$
    \item Bias: $b = 0.2$
    \item Activation: sigmoid
\end{itemize}

Compute the output.

\subsection{Problem 2: Network Architecture}

Design a network architecture for:
\begin{itemize}
    \item Input: 784-dimensional vector (28×28 image)
    \item Output: 10 classes (digits 0-9)
    \item Constraint: Use 2 hidden layers
\end{itemize}

Specify dimensions and activation functions.

\subsection{Problem 3: Activation Functions}

Why can't we use only sigmoid activations in very deep networks? What problems arise?

\subsection{Problem 4: Universal Approximation}

If neural networks can approximate any function, why do we need deep networks? Why not use one wide hidden layer?

\section{Key Takeaways}

\begin{itemize}
    \item \textbf{Neural networks} are compositions of linear transforms and nonlinear activations
    \item \textbf{Activation functions} introduce nonlinearity for complex function learning
    \item \textbf{ReLU} is the most common activation due to good gradients
    \item \textbf{Feedforward networks} stack layers to build deep representations
    \item \textbf{Cross-entropy loss} is standard for classification
    \item \textbf{Softmax} converts logits to probability distributions
    \item \textbf{Universal approximation} guarantees representational power
    \item Understanding forward propagation is essential before backpropagation
\end{itemize}

\begin{connection}
You now understand the mathematical building blocks of neural networks! In the next chapter, we'll learn how networks actually learn—through backpropagation, the algorithm that computes gradients efficiently using the chain rule. This is where calculus and neural networks unite!
\end{connection}
