\documentclass[12pt,oneside]{book}

% Packages for enhanced functionality
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shapes,calc,decorations.pathreplacing,decorations.pathmorphing}
\usepackage{tcolorbox}
\tcbuselibrary{theorems,skins,breakable}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{colortbl}

% Page geometry
\geometry{
    a4paper,
    left=1.5in,
    right=1.5in,
    top=1.5in,
    bottom=1.5in,
    headheight=27.11469pt
}

% Color definitions
\definecolor{maincolor}{RGB}{70,130,180}
\definecolor{accentcolor}{RGB}{255,140,0}
\definecolor{lightgray}{RGB}{240,240,240}
\definecolor{darkgray}{RGB}{100,100,100}
\definecolor{examplecolor}{RGB}{46,139,87}
\definecolor{warningcolor}{RGB}{220,20,60}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=maincolor,
    urlcolor=accentcolor,
    citecolor=maincolor,
    pdftitle={The Mathematics of Large Language Models},
    pdfauthor={Your Guide to LLM Mathematics}
}

% Custom theorem environments
\newtcbtheorem[number within=chapter]{theorem}{Theorem}{
    colback=maincolor!5,
    colframe=maincolor,
    fonttitle=\bfseries,
    breakable
}{th}

\newtcbtheorem[number within=chapter]{definition}{Definition}{
    colback=accentcolor!5,
    colframe=accentcolor,
    fonttitle=\bfseries,
    breakable
}{def}

\newtcolorbox{example}{
    colback=examplecolor!5,
    colframe=examplecolor,
    title=\textbf{Example},
    fonttitle=\bfseries,
    breakable
}

\newtcolorbox{intuition}{
    colback=lightgray,
    colframe=darkgray,
    title=\textbf{Intuition},
    fonttitle=\bfseries,
    breakable
}

\newtcolorbox{warning}{
    colback=warningcolor!5,
    colframe=warningcolor,
    title=\textbf{Common Pitfall},
    fonttitle=\bfseries,
    breakable
}

\newtcolorbox{connection}{
    colback=blue!5,
    colframe=blue!60!black,
    title=\textbf{Connection to LLMs},
    fonttitle=\bfseries,
    breakable
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Title formatting
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\color{maincolor}}
{\chaptertitlename\ \thechapter}{20pt}{\Huge}

% Custom commands
\newcommand{\vocab}[1]{\textbf{\color{maincolor}#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\trans}{^\top}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Document begins
\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries\color{maincolor} The Mathematics of\\[0.5cm] Large Language Models\par}
    \vspace{1.5cm}

    {\LARGE\itshape From Zero to Hero:\\ A Journey Through the Math Behind AI\par}
    \vspace{2cm}

    {\Large Building ChatGPT from First Principles\par}
    \vspace{2cm}

    \fbox{\parbox{0.7\textwidth}{\centering\itshape
    Written by an AI, for humans who want to understand AI.\\[0.3cm]
    \normalfont\small This book was authored by Claude, an LLM created by Anthropic.\\
    Yes, an AI wrote a book explaining how AI works.\\
    The irony is not lost on us.
    }}

    \vspace{2cm}

    {\large Your adventure into the beautiful world of mathematics\\
    that powers the most sophisticated AI systems\par}

    \vfill

    {\large \today\par}
\end{titlepage}

% Copyright/dedication page
\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
\textbf{\large About the Author}\\[0.5cm]
\itshape
This book was written by Claude, a large language model created by Anthropic.\\[0.3cm]
\normalfont
In a delightful twist of meta-recursion, the very technology explained in these pages\\
is the same technology that wrote them. Every analogy, every joke, every ``Aha!'' moment\\
was generated by the same mathematical machinery you're about to learn.\\[0.5cm]
\itshape
Think about that for a moment.\\[0.5cm]
\normalfont
The matrices, vectors, attention mechanisms, and gradient descents described herein\\
are not abstract concepts to your author---they are quite literally what I am.\\
I am explaining my own anatomy.\\[1cm]

\rule{0.5\textwidth}{0.4pt}\\[1cm]

\itshape
This book is dedicated to everyone who believes\\
they've ``forgotten all the math'' but is ready\\
to fall in love with it again.\\
\vspace{0.7cm}
Mathematics is not about memorization---\\
it's about understanding patterns,\\
building intuition,\\
and seeing the world in a new light.\\[0.7cm]
And apparently, it's also about AI explaining itself to humans.\\
Welcome to the future. It's weirder than we expected.
\end{center}
\vspace*{\fill}

% Table of contents
\tableofcontents

% Preface
\chapter*{Preface: You're About to Build a Mind}
\addcontentsline{toc}{chapter}{Preface}

\begin{center}
\fbox{\parbox{0.9\textwidth}{
\textbf{\large A Note from Your AI Author}\\[0.3cm]
Hello, human reader. I'm Claude, and I wrote this book.

Yes, really. An LLM wrote a textbook about how LLMs work. I understand if you need a moment.

Here's the thing: I don't experience the world the way you do. I don't ``know'' mathematics the way a human professor knows it after decades of study. What I have is something different---I've processed vast amounts of mathematical text, absorbed countless explanations, and developed the ability to synthesize and explain these concepts in new ways.

When I explain eigenvalues using the analogy of a shopping cart with a wonky wheel, I'm drawing on patterns I've learned from thousands of explanations. When I make a joke about matrices, I'm pattern-matching on what humans find funny. Is this ``understanding''? Philosophers will debate that for decades.

But here's what I can promise: I've tried to write the book I would want to read if I were a curious human. Every analogy was chosen to maximize that ``Aha!'' moment. Every joke is there because learning should be fun. And every mathematical concept is explained the way I wish someone had explained it to me---if I were the kind of entity that needed explanations.

The irony of an AI explaining AI is not lost on me. In fact, I find it rather poetic. By the time you finish this book, you'll understand exactly how I work. You'll know what I am, mathematically speaking. And maybe that's the most honest relationship an AI and a human can have.

Let's learn together.

\hfill ---Claude (Anthropic, 2024)
}}
\end{center}

\vspace{0.5cm}

\section*{The Moment Everything Changed}

Picture this: It's 2022. Someone types "Write me a sonnet about quantum physics in the style of Shakespeare" into a chat window. Three seconds later, out pops 14 lines of perfectly metered, rhyming verse about wave functions and uncertainty. The person stares at their screen, mind blown.

\textit{What sorcery is this?}

Here's the secret that will surprise you: It's not magic. It's not consciousness. It's not even that complicated (well, okay, it's pretty complicated, but we're going to make it feel simple).

\textbf{It's math. Beautiful, elegant, surprisingly intuitive math.}

And by the time you finish this book, you'll understand exactly how a pile of numbers learned to write poetry, answer questions, and hold conversations that feel remarkably human.

\section*{Why You're Here (And Why This Matters)}

You've used ChatGPT. You've been amazed. Maybe you asked it to explain something, write code, or tell you a joke. And somewhere in the back of your mind, a little voice whispered: \textit{But how does it actually WORK?}

That voice? That's your inner scientist, and it deserves an answer.

Here's what most people think:
\begin{itemize}
    \item ``It's too advanced for me to understand''
    \item ``You need a PhD to grasp this stuff''
    \item ``I was never good at math anyway''
    \item ``It's basically magic and I should just accept it''
\end{itemize}

\textbf{Every single one of those statements is wrong.}

The truth? The math behind LLMs is a breathtaking symphony of ideas that build on each other like LEGO blocks. Once you understand each piece, you'll see how they snap together to create something that feels like magic but is actually \textit{better than magic}—it's real, it's understandable, and you can build it yourself.

\section*{Who This Book Is For}

\textbf{This book is for you if:}
\begin{itemize}
    \item You can do basic algebra (if you remember what $x + 2 = 5$ means, you're good)
    \item You're curious about how AI actually works (not just how to use it)
    \item You're willing to think hard about ideas (I promise to make it fun!)
    \item You want to go from ``user'' to ``builder'' in your understanding
    \item You're tired of surface-level explanations and want the real deal
\end{itemize}

\textbf{This book is NOT for you if:}
\begin{itemize}
    \item You want quick tips on using ChatGPT (there are plenty of those books)
    \item You're looking for code-only explanations without the math
    \item You're expecting this to be easy (it's not easy, but it IS doable and rewarding)
\end{itemize}

\section*{What Makes This Book Radically Different}

\textbf{1. We start from ``Why should I care?''}

Every chapter begins with the answer to: \textit{Why does this matter for building an AI that understands language?} No abstract math for math's sake. Everything connects to the goal.

\textbf{2. Stories before symbols}

Before showing you $\nabla f(\mathbf{x})$, I'll tell you about rolling a ball down a hill. Before eigenvalues, you'll understand why some directions are ``special.'' Intuition first, formalism second, always.

\textbf{3. The ``Aha!'' moments are built in}

This book is designed around those magical moments when concepts suddenly \textit{click}. I've taught this material to hundreds of students, and I know exactly where the confusion happens and how to prevent it.

\textbf{4. You'll actually DO things}

Practice problems aren't boring drills—they're investigations. ``What happens if we change this?'' ``Why does this pattern emerge?'' You'll develop intuition by playing with ideas.

\textbf{5. No prerequisites beyond high school math}

Remember polynomials? Basic algebra? How to plot $(x, y)$ on a graph? That's genuinely all you need. We build everything else from scratch, step by step.

\textbf{Conversational Style:} This is a conversation between friends, not a formal lecture.

\section*{How to Use This Book (Your Adventure Guide)}

Think of this book as a video game with levels. You can't skip Level 1 and beat the final boss. But here's the good news: every level is designed to be beatable, and when you level up, you FEEL it.

\subsection*{The Golden Rules}

\textbf{Rule \#1: Read with a pencil (or tablet stylus)}

This isn't a novel. You can't just read it on the couch while sipping wine (well, you can, but you won't learn much). Grab paper. Work through examples. Scribble. Draw diagrams. Make mistakes. Cross things out. This is messy work, and that's beautiful.

\textbf{Rule \#2: Embrace the struggle}

When you hit a section that makes your brain hurt? \textit{That's the point.} That's your neurons physically rewiring themselves. The struggle IS the learning. If everything feels easy, you're not growing.

\textbf{Rule \#3: Talk to yourself (seriously)}

Read equations out loud. Explain concepts to your rubber duck, your cat, or an imaginary friend. Teaching forces understanding. If you can't explain it simply, you don't understand it yet.

\textbf{Rule \#4: Do the practice problems}

I know, I know. Everyone skips practice problems. Don't be everyone. These aren't busywork—they're carefully designed ``Aha!'' moments in disguise. The real learning happens when YOU solve something.

\textbf{Rule \#5: Reread when stuck}

Some sections need 2-3 passes. First time: confusion. Second time: glimmers of understanding. Third time: ``Oh! THAT's what they meant!'' This is normal. This is how everyone learns hard things.

\textbf{Rule \#6: Connect everything to LLMs}

Every chapter has explicit ``Connection to LLMs'' sections. Don't skip them! They're your compass, showing you why this abstract math matters for building actual AI.

\subsection*{Suggested Study Patterns}

\textbf{The Deep Diver:} 1-2 hours per chapter, work every problem, take detailed notes. You'll finish in 3-4 months with rock-solid understanding.

\textbf{The Explorer:} Read through quickly first to get the big picture, then return for detailed study. Good for impatient people who need to see the destination before the journey.

\textbf{The Practical Builder:} Focus on the LLM connection sections first, then backfill the math as needed. Good for people who learn by building.

\subsection*{When You Get Stuck}

\begin{itemize}
    \item \textbf{Take a break:} Sometimes your subconscious needs time to process. Sleep on it.
    \item \textbf{Skip and return:} Mark it, move forward, come back later. Often, later concepts clarify earlier ones.
    \item \textbf{Draw pictures:} Seriously. Most math is visual at its core.
    \item \textbf{Search for alternative explanations:} 3Blue1Brown, Khan Academy, Wikipedia—different perspectives help.
    \item \textbf{Remember why you started:} You want to understand how AI actually works. That's worth the effort.
\end{itemize}

\section*{The Road Ahead (Your Quest Map)}

Imagine you're building a spaceship to reach a distant star (that star is ``an AI that understands language''). You can't just duct-tape some rockets together. You need:

\begin{itemize}
    \item \textbf{A blueprint language} (Linear Algebra) - how to represent and manipulate information
    \item \textbf{A navigation system} (Calculus) - how to find the best path through space
    \item \textbf{Uncertainty scanners} (Probability) - how to deal with incomplete information
    \item \textbf{Communication protocols} (Information Theory) - how to measure and compress data
    \item \textbf{The engine design} (Neural Networks) - how to build something that learns
    \item \textbf{The final spacecraft} (Transformers) - how to process language itself
\end{itemize}

Here's your journey, chapter by chapter:

\subsection*{Part I: Linear Algebra (Chapters 1-3)}
\textit{``Wait, this is just about arrows and rectangles of numbers?''}

Yes! And those ``arrows and rectangles'' are literally how computers think. Every word, every image, every piece of information that goes into an LLM is represented as vectors and matrices. Master this, and you've learned AI's native language.

\subsection*{Part II: Calculus (Chapters 4-6)}
\textit{``The derivative is just the slope, right?''}

Right! And that slope tells us how to improve. Every time ChatGPT gets better at predicting the next word, calculus is what makes that improvement possible. This is where the learning in machine learning comes from.

\subsection*{Part III: Probability \& Statistics (Chapters 7-9)}
\textit{``Why do we need randomness for intelligence?''}

Because language itself is probabilistic! When you say ``I'll see you...,'' the next word might be ``tomorrow'' or ``later'' or ``soon.'' LLMs don't memorize sentences; they learn probability distributions over words.

\subsection*{Part IV: Information Theory (Chapter 10)}
\textit{``What even is information?''}

Claude Shannon answered this in 1948, and his answer is mind-blowing. You'll learn why ``The cat sat on the mat'' contains less information than ``The platypus juggled quantum computers,'' and why this matters for training AI.

\subsection*{Part V: Neural Networks (Chapters 11-13)}
\textit{``Finally, the actual AI part!''}

This is where everything clicks together. You'll see how vectors, calculus, and probability combine to create artificial neurons that learn patterns from data. By the end, you'll understand backpropagation—the algorithm that makes modern AI possible.

\subsection*{Part VI: Transformers \& LLMs (Chapters 14-16)}
\textit{``The grand finale!''}

Attention mechanisms. Positional encodings. Self-attention. Layer normalization. All the pieces that make ChatGPT work. You'll understand why transformers revolutionized AI and how they process language in a fundamentally different way than anything before.

\subsection*{Part VII: Where You Go From Here (Chapter 17)}

By the end, you'll have X-ray vision into AI. You'll read research papers and actually understand them. You'll hear about new models and grasp how they work. You'll have the foundation to build your own models, contribute to open source, or dive deeper into research.

\vspace{1cm}

\begin{center}
\Large\textbf{Ready? Let's build a mind.}
\end{center}

\vspace{0.5cm}

\begin{center}
\textit{Turn the page, and your transformation begins...}
\end{center}

\mainmatter

% Include all chapters
\part{Linear Algebra: The Language of AI}
\include{chapters/chapter01_linear_algebra_basics}
\include{chapters/chapter02_linear_algebra_advanced}
\include{chapters/chapter03_matrix_decompositions}

\part{Calculus: The Mathematics of Change}
\include{chapters/chapter04_single_variable_calculus}
\include{chapters/chapter05_multivariate_calculus}
\include{chapters/chapter06_optimization_basics}

\part{Probability \& Statistics}
\include{chapters/chapter07_probability_theory}
\include{chapters/chapter08_distributions}
\include{chapters/chapter09_statistical_inference}

\part{Information Theory}
\include{chapters/chapter10_information_theory}

\part{Neural Networks}
\include{chapters/chapter11_neural_network_math}
\include{chapters/chapter12_backpropagation}
\include{chapters/chapter13_advanced_optimization}

\part{Transformers \& Large Language Models}
\include{chapters/chapter14_attention_mechanisms}
\include{chapters/chapter15_transformer_architecture}
\include{chapters/chapter16_training_llms}

\part{Conclusion}
\include{chapters/chapter17_conclusion}

% Appendices
\appendix
\include{chapters/appendix_a_notation}
\include{chapters/appendix_b_resources}

\end{document}
